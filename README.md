Name: Lnu Rumana Thaskeen Student ID: 700742859 CRN: 23216

Here is the link to my Document with description : https://docs.google.com/document/d/1MOQ_UQRr7oKhO5_zYYfA7gqUmzjmPiTT/edit?usp=sharing&ouid=104224780424464634499&rtpof=true&sd=true

Here is the link to my Video : https://drive.google.com/file/d/1ug75pUEzA3zTxuO3qtuG5g4z0PL5Ze8K/view?usp=sharing


2. Provide logical description of which steps lead to improved response and what was its impact on
architecture behavior

There were several changes made to the baseline LeNet model to improve its performance on the CIFAR-10 dataset. Here is a description of the changes made and their impact:

Data Augmentation: Data augmentation techniques were applied to the training dataset to artificially increase its size and diversity. This helped the model generalize better to new, unseen images and reduce overfitting. The impact of data augmentation was seen in the improved validation accuracy and lower validation loss.

Learning Rate Scheduler: A learning rate scheduler was added to the model to reduce the learning rate over time as the model gets closer to convergence. This helped the model to converge faster and improve its performance by reducing the oscillations in the loss and accuracy. The impact of this was seen in the smoother convergence and faster training time.

Batch Normalization: Batch normalization layers were added to the model to improve its stability and reduce internal covariate shift. This helped to stabilize the gradient flow during training and led to faster convergence and improved generalization performance. The impact of this was seen in the improved validation accuracy and lower validation loss.

Dropout: Dropout layers were added to the model to randomly drop out some of the neurons during training. This helped to prevent overfitting by forcing the model to learn more robust features that generalize better to new, unseen images. The impact of this was seen in the improved validation accuracy and lower validation loss.

Overall, these changes led to a significant improvement in the model's performance, with a validation accuracy of over 80% and a validation loss of less than 0.7. The improved architecture was able to learn more robust features that generalize better to new images and reduce overfitting.


11. To improve the response of a model for a new dataset, the following logical steps have been taken in this ICP:

Data Preprocessing: The first step is to preprocess the new dataset by cleaning, normalizing, and transforming the data into a suitable format for the model. This includes techniques like removing outliers, imputing missing values, and feature scaling.

Baseline Model: Train a baseline model on the preprocessed dataset. This model serves as a starting point for comparison with the improved architecture.

Performance Evaluation: Evaluate the performance of the baseline model on the new dataset using appropriate metrics like accuracy, precision, recall, F1-score, etc.

Identify Model Weaknesses: Analyze the performance of the baseline model and identify its weaknesses. This can be done by studying the confusion matrix, visualizing the data, or conducting statistical tests.

Enhance Architecture: Based on the weaknesses identified, enhance the architecture by adding more layers, increasing the number of neurons, using a different activation function, or adopting a different optimization technique. This process can be done through trial and error or using automated techniques like hyperparameter tuning.

Performance Evaluation: Train the enhanced architecture on the preprocessed dataset and evaluate its performance using the same metrics as the baseline model.

Compare Results: Compare the results of the enhanced architecture with the baseline model. If the enhanced architecture performs better, then it can be selected as the final model for the new dataset.

The impact of enhancing the architecture on its behavior depends on the specific changes made. Adding more layers or neurons can increase the model's complexity and capacity, allowing it to capture more complex relationships in the data. Using a different activation function or optimization technique can change the model's convergence behavior and improve its ability to find the optimal solution. Overall, enhancing the architecture can improve the model's accuracy, precision, recall, and F1-score on the new dataset.


To improve the response for a new dataset on LeNet, AlexNet, VGG16, and VGG19, the following steps have been taken:

Data pre-processing: The first step is to pre-process the new dataset by performing tasks such as normalization, data augmentation, and image resizing to ensure that the dataset is ready for training.

Baseline model training: The next step is to train a baseline model on the new dataset using LeNet, AlexNet, VGG16, or VGG19. This will provide a baseline performance that can be used to compare the performance of the enhanced models.

Analysis of baseline model performance: The performance of the baseline model should be analyzed to identify areas where it can be improved. For example, the model may be underfitting or overfitting, or it may have high bias or variance.

Enhance architecture: Based on the analysis of the baseline model performance, the architecture can be enhanced by adding more layers, increasing the number of filters, or changing the activation function. This can improve the model's ability to learn more complex features from the data.

Impact analysis: The impact of the enhanced architecture on the model's behavior can be analyzed by evaluating its performance on the validation and test sets. This will help determine if the enhanced architecture has improved the model's ability to generalize to new data.

Overall, enhancing the architecture gave a significant impact on the performance of the model, improving its accuracy and ability to generalize to new data. However, it is important to carefully analyze the baseline model's performance before making changes to the architecture to ensure that the changes are effective.
![image](https://user-images.githubusercontent.com/122562147/228840676-5053819b-e539-447f-b681-5d99467d5aae.png)



